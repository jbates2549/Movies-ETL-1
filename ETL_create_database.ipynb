{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "from config import db_password\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Add the clean movie function that takes in the argument, \"movie\".\n",
    "def clean_movie(movie):\n",
    "    \n",
    "    movie = dict(movie) #create a non-destructive copy\n",
    "    alt_titles = {}\n",
    "    # combine alternate titles into one list\n",
    "    for key in ['Also known as','Arabic','Cantonese','Chinese','French',\n",
    "                'Hangul','Hebrew','Hepburn','Japanese','Literally',\n",
    "                'Mandarin','McCune-Reischauer','Original title','Polish',\n",
    "                'Revised Romanization','Romanized','Russian',\n",
    "                'Simplified','Traditional','Yiddish']:\n",
    "        if key in movie:\n",
    "            alt_titles[key] = movie[key]\n",
    "            movie.pop(key)\n",
    "    if len(alt_titles) > 0:\n",
    "        movie['alt_titles'] = alt_titles\n",
    "\n",
    "    # merge column names\n",
    "    def change_column_name(old_name, new_name):\n",
    "        if old_name in movie:\n",
    "            movie[new_name] = movie.pop(old_name)\n",
    "    change_column_name('Adaptation by', 'Writer(s)')\n",
    "    change_column_name('Country of origin', 'Country')\n",
    "    change_column_name('Directed by', 'Director')\n",
    "    change_column_name('Distributed by', 'Distributor')\n",
    "    change_column_name('Edited by', 'Editor(s)')\n",
    "    change_column_name('Length', 'Running time')\n",
    "    change_column_name('Original release', 'Release date')\n",
    "    change_column_name('Music by', 'Composer(s)')\n",
    "    change_column_name('Produced by', 'Producer(s)')\n",
    "    change_column_name('Producer', 'Producer(s)')\n",
    "    change_column_name('Productioncompanies ', 'Production company(s)')\n",
    "    change_column_name('Productioncompany ', 'Production company(s)')\n",
    "    change_column_name('Released', 'Release Date')\n",
    "    change_column_name('Release Date', 'Release date')\n",
    "    change_column_name('Screen story by', 'Writer(s)')\n",
    "    change_column_name('Screenplay by', 'Writer(s)')\n",
    "    change_column_name('Story by', 'Writer(s)')\n",
    "    change_column_name('Theme music composer', 'Composer(s)')\n",
    "    change_column_name('Written by', 'Writer(s)')\n",
    "\n",
    "    return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Add the function that takes in three arguments;\n",
    "def extract_transform_load():\n",
    "    # Read in the kaggle metadata and MovieLens ratings CSV files as Pandas DataFrames.\n",
    "    \n",
    "    kaggle_metadata = pd.read_csv('movies_metadata.csv', low_memory=False)\n",
    "    ratings = pd.read_csv('ratings.csv')   \n",
    "\n",
    "    # Open and read the Wikipedia data JSON file.\n",
    "    \n",
    "    with open('wikipediamovies.json', mode='r') as file:\n",
    "        wiki_movies_raw = json.load(file)\n",
    "    \n",
    "    # 3. Write a list comprehension to filter out TV shows.\n",
    "    \n",
    "    wiki_movies = [movie for movie in wiki_movies_raw\n",
    "                  if ('No. of episodes' not in movie)]\n",
    "\n",
    "    # 4. Write a list comprehension to iterate through the cleaned wiki movies list\n",
    "    # and call the clean_movie function on each movie.\n",
    "    \n",
    "    clean_movies = [clean_movie(movie) for movie in wiki_movies]\n",
    "\n",
    "    # 5. Read in the cleaned movies list from Step 4 as a DataFrame.\n",
    "    \n",
    "    wiki_movies_df = pd.DataFrame(clean_movies)\n",
    "\n",
    "\n",
    "    # 6. Write a try-except block to catch errors while extracting the IMDb ID using a regular expression string and\n",
    "    #  dropping any imdb_id duplicates. If there is an error, capture and print the exception.\n",
    "    try:\n",
    "        wiki_movies_df['imdb_id'] = wiki_movies_df['imdb_link'].str.extract(r'(tt\\d{7})')\n",
    "        wiki_movies_df.drop_duplicates(subset='imdb_id', inplace=True)   \n",
    "    except Exception as e: print(e)\n",
    "        \n",
    "\n",
    "    #  7. Write a list comprehension to keep the columns that don't have null values from the wiki_movies_df DataFrame.\n",
    "    wiki_columns_to_keep = [column for column in wiki_movies_df.columns if wiki_movies_df[column].isnull().sum() < len(wiki_movies_df) * 0.9]\n",
    "    wiki_movies_df = wiki_movies_df[wiki_columns_to_keep]\n",
    "\n",
    "    # 8. Create a variable that will hold the non-null values from the “Box office” column.\n",
    "    box_office = wiki_movies_df['Box office'].dropna()\n",
    "    \n",
    "    # 9. Convert the box office data created in Step 8 to string values using the lambda and join functions.\n",
    "    box_office = box_office.apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "\n",
    "    # 10. Write a regular expression to match the six elements of \"form_one\" of the box office data.\n",
    "    form_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on'\n",
    "    \n",
    "    # 11. Write a regular expression to match the three elements of \"form_two\" of the box office data.\n",
    "    form_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)'\n",
    "\n",
    "    # 12. Add the parse_dollars function.\n",
    "    \n",
    "    def parse_dollars(s):\n",
    "    # if s is not a string, return NaN\n",
    "        if type(s) != str:\n",
    "            return np.nan\n",
    "\n",
    "    # if input is of the form $###.# million\n",
    "        if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*milli?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "        # remove dollar sign and \" million\"\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "        # convert to float and multiply by a million\n",
    "            value = float(s) * 10**6\n",
    "\n",
    "        # return value\n",
    "            return value\n",
    "\n",
    "    # if input is of the form $###.# billion\n",
    "        elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*billi?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "        # remove dollar sign and \" billion\"\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "        # convert to float and multiply by a billion\n",
    "            value = float(s) * 10**9\n",
    "\n",
    "        # return value\n",
    "            return value\n",
    "\n",
    "    # if input is of the form $###,###,###\n",
    "        elif re.match(r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)', s, flags=re.IGNORECASE):\n",
    "\n",
    "        # remove dollar sign and commas\n",
    "            s = re.sub('\\$|,','', s)\n",
    "\n",
    "        # convert to float\n",
    "            value = float(s)\n",
    "\n",
    "        # return value\n",
    "            return value\n",
    "\n",
    "    # otherwise, return NaN\n",
    "        else:\n",
    "            return np.nan\n",
    "        \n",
    "    # 13. Clean the box office column in the wiki_movies_df DataFrame.\n",
    "\n",
    "    wiki_movies_df['box_office'] = box_office.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    wiki_movies_df.drop('Box office', axis=1, inplace=True)\n",
    "    \n",
    "    # 14. Clean the budget column in the wiki_movies_df DataFrame.\n",
    "    budget = wiki_movies_df['Budget'].dropna()\n",
    "    budget = budget.map(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    budget = budget.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)\n",
    "    budget = budget.str.replace(r'\\[\\d+\\]\\s*', '')\n",
    "    form_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on'\n",
    "    form_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)'\n",
    "    wiki_movies_df['budget'] = budget.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    wiki_movies_df.drop('Budget', axis=1, inplace=True)\n",
    "    \n",
    "    # 15. Clean the release date column in the wiki_movies_df DataFrame.\n",
    "    release_date = wiki_movies_df['Release date'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    date_form_one = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s[123]\\d,\\s\\d{4}'\n",
    "    date_form_two = r'\\d{4}.[01]\\d.[123]\\d'\n",
    "    date_form_three = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{4}'\n",
    "    date_form_four = r'\\d{4}'\n",
    "    release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})', flags=re.IGNORECASE)\n",
    "    wiki_movies_df['release_date'] = pd.to_datetime(release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})')[0], infer_datetime_format=True)\n",
    "    \n",
    "    # 16. Clean the running time column in the wiki_movies_df DataFrame.\n",
    "    running_time = wiki_movies_df['Running time'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    running_time_extract = running_time.str.extract(r'(\\d+)\\s*ho?u?r?s?\\s*(\\d*)|(\\d+)\\s*m')\n",
    "    running_time_extract = running_time_extract.apply(lambda col: pd.to_numeric(col, errors='coerce')).fillna(0)\n",
    "    wiki_movies_df['running_time'] = running_time_extract.apply(lambda row: row[0]*60 + row[1] if row[2] == 0 else row[2], axis=1)\n",
    "    wiki_movies_df.drop('Running time', axis=1, inplace=True)\n",
    "    \n",
    "     \n",
    "    # 2. Clean the Kaggle metadata.\n",
    "    kaggle_metadata = kaggle_metadata[kaggle_metadata['adult'] == 'False'].drop('adult',axis='columns')\n",
    "    kaggle_metadata['video'] = kaggle_metadata['video'] == 'True'\n",
    "    kaggle_metadata['budget'] = kaggle_metadata['budget'].astype(int)\n",
    "    kaggle_metadata['id'] = pd.to_numeric(kaggle_metadata['id'], errors='raise')\n",
    "    kaggle_metadata['popularity'] = pd.to_numeric(kaggle_metadata['popularity'], errors='raise')\n",
    "    kaggle_metadata['release_date'] = pd.to_datetime(kaggle_metadata['release_date'])\n",
    "    \n",
    "    # 3. Merged the two DataFrames into the movies DataFrame.\n",
    "    movies_df = pd.merge(wiki_movies_df, kaggle_metadata, on='imdb_id', suffixes=['_wiki','_kaggle'])\n",
    "    \n",
    "    # 4. Drop unnecessary columns from the merged DataFrame.\n",
    "    movies_df.drop(columns=['title_wiki','release_date_wiki','Language','Production company(s)'], inplace=True)\n",
    "    \n",
    "    # 5. Add in the function to fill in the missing Kaggle data.\n",
    "\n",
    "    def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "        df[kaggle_column] = df.apply(\n",
    "            lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column]\n",
    "            , axis=1)\n",
    "        df.drop(columns=wiki_column, inplace=True)\n",
    "        \n",
    "    # 6. Call the function in Step 5 with the DataFrame and columns as the arguments.\n",
    "    fill_missing_kaggle_data(movies_df, 'runtime', 'running_time')\n",
    "    fill_missing_kaggle_data(movies_df, 'budget_kaggle', 'budget_wiki')\n",
    "    fill_missing_kaggle_data(movies_df, 'revenue', 'box_office')\n",
    "    movies_df\n",
    "\n",
    "    # 7. Filter the movies DataFrame for specific columns.\n",
    "    movies_df = movies_df.loc[:, ['imdb_id','id','title_kaggle','original_title','tagline','belongs_to_collection','url','imdb_link',\n",
    "                       'runtime','budget_kaggle','revenue','release_date_kaggle','popularity','vote_average','vote_count',\n",
    "                       'genres','original_language','overview','spoken_languages','Country',\n",
    "                       'production_companies','production_countries','Distributor',\n",
    "                       'Producer(s)','Director','Starring','Cinematography','Editor(s)','Writer(s)','Composer(s)','Based on'\n",
    "                      ]]\n",
    "\n",
    "    # 8. Rename the columns in the movies DataFrame.\n",
    "    movies_df.rename({'id':'kaggle_id',\n",
    "                  'title_kaggle':'title',\n",
    "                  'url':'wikipedia_url',\n",
    "                  'budget_kaggle':'budget',\n",
    "                  'release_date_kaggle':'release_date',\n",
    "                  'Country':'country',\n",
    "                  'Distributor':'distributor',\n",
    "                  'Producer(s)':'producers',\n",
    "                  'Director':'director',\n",
    "                  'Starring':'starring',\n",
    "                  'Cinematography':'cinematography',\n",
    "                  'Editor(s)':'editors',\n",
    "                  'Writer(s)':'writers',\n",
    "                  'Composer(s)':'composers',\n",
    "                  'Based on':'based_on'\n",
    "                 }, axis='columns', inplace=True)\n",
    "\n",
    "    # 9. Transform and merge the ratings DataFrame.\n",
    "    \n",
    "    rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "                .rename({'userId':'count'}, axis=1) \\\n",
    "                .pivot(index='movieId',columns='rating', values='count')\n",
    "    rating_counts.columns = ['rating_' + str(col) for col in rating_counts.columns]\n",
    "    movies_with_ratings_df = pd.merge(movies_df, rating_counts, left_on='kaggle_id', right_index=True, how='left')\n",
    "    \n",
    "    db_string = f\"postgres://postgres:{db_password}@127.0.0.1:5432/movie_data\"\n",
    "    \n",
    "    engine = create_engine(db_string)\n",
    "    \n",
    "    movies_df.to_sql(name='movies', con=engine,if_exists='replace')\n",
    "    \n",
    "    return wiki_movies_df,movies_with_ratings_df,movies_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1 Add the function that takes in three arguments;\n",
    "# def extract_transform_load():\n",
    "#     # Read in the kaggle metadata and MovieLens ratings CSV files as Pandas DataFrames.\n",
    "    \n",
    "#     kaggle_metadata = pd.read_csv('movies_metadata.csv', low_memory=False)\n",
    "#     ratings = pd.read_csv('ratings.csv')   \n",
    "\n",
    "#     # Open and read the Wikipedia data JSON file.\n",
    "    \n",
    "#     with open('wikipediamovies.json', mode='r') as file:\n",
    "#         wiki_movies_raw = json.load(file)\n",
    "    \n",
    "#     # 3. Write a list comprehension to filter out TV shows.\n",
    "    \n",
    "#     wiki_movies = [movie for movie in wiki_movies_raw\n",
    "#                   if ('No. of episodes' not in movie)]\n",
    "\n",
    "#     # 4. Write a list comprehension to iterate through the cleaned wiki movies list\n",
    "#     # and call the clean_movie function on each movie.\n",
    "    \n",
    "#     clean_movies = [clean_movie(movie) for movie in wiki_movies]\n",
    "\n",
    "#     # 5. Read in the cleaned movies list from Step 4 as a DataFrame.\n",
    "    \n",
    "#     wiki_movies_df = pd.DataFrame(clean_movies)\n",
    "\n",
    "\n",
    "#     # 6. Write a try-except block to catch errors while extracting the IMDb ID using a regular expression string and\n",
    "#     #  dropping any imdb_id duplicates. If there is an error, capture and print the exception.\n",
    "#     try:\n",
    "#         wiki_movies_df['imdb_id'] = wiki_movies_df['imdb_link'].str.extract(r'(tt\\d{7})')\n",
    "#         wiki_movies_df.drop_duplicates(subset='imdb_id', inplace=True)   \n",
    "#     except Exception as e: print(e)\n",
    "        \n",
    "\n",
    "#     #  7. Write a list comprehension to keep the columns that don't have null values from the wiki_movies_df DataFrame.\n",
    "#     wiki_columns_to_keep = [column for column in wiki_movies_df.columns if wiki_movies_df[column].isnull().sum() < len(wiki_movies_df) * 0.9]\n",
    "#     wiki_movies_df = wiki_movies_df[wiki_columns_to_keep]\n",
    "\n",
    "#     # 8. Create a variable that will hold the non-null values from the “Box office” column.\n",
    "#     box_office = wiki_movies_df['Box office'].dropna()\n",
    "    \n",
    "#     # 9. Convert the box office data created in Step 8 to string values using the lambda and join functions.\n",
    "#     box_office = box_office.apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "\n",
    "#     # 10. Write a regular expression to match the six elements of \"form_one\" of the box office data.\n",
    "#     form_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on'\n",
    "    \n",
    "#     # 11. Write a regular expression to match the three elements of \"form_two\" of the box office data.\n",
    "#     form_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)'\n",
    "\n",
    "#     # 12. Add the parse_dollars function.\n",
    "    \n",
    "#     def parse_dollars(s):\n",
    "#     # if s is not a string, return NaN\n",
    "#         if type(s) != str:\n",
    "#             return np.nan\n",
    "\n",
    "#     # if input is of the form $###.# million\n",
    "#         if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*milli?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "#         # remove dollar sign and \" million\"\n",
    "#             s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "#         # convert to float and multiply by a million\n",
    "#             value = float(s) * 10**6\n",
    "\n",
    "#         # return value\n",
    "#             return value\n",
    "\n",
    "#     # if input is of the form $###.# billion\n",
    "#         elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*billi?on', s, flags=re.IGNORECASE):\n",
    "\n",
    "#         # remove dollar sign and \" billion\"\n",
    "#             s = re.sub('\\$|\\s|[a-zA-Z]','', s)\n",
    "\n",
    "#         # convert to float and multiply by a billion\n",
    "#             value = float(s) * 10**9\n",
    "\n",
    "#         # return value\n",
    "#             return value\n",
    "\n",
    "#     # if input is of the form $###,###,###\n",
    "#         elif re.match(r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)', s, flags=re.IGNORECASE):\n",
    "\n",
    "#         # remove dollar sign and commas\n",
    "#             s = re.sub('\\$|,','', s)\n",
    "\n",
    "#         # convert to float\n",
    "#             value = float(s)\n",
    "\n",
    "#         # return value\n",
    "#             return value\n",
    "\n",
    "#     # otherwise, return NaN\n",
    "#         else:\n",
    "#             return np.nan\n",
    "        \n",
    "#     # 13. Clean the box office column in the wiki_movies_df DataFrame.\n",
    "\n",
    "#     wiki_movies_df['box_office'] = box_office.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "#     wiki_movies_df.drop('Box office', axis=1, inplace=True)\n",
    "    \n",
    "#     # 14. Clean the budget column in the wiki_movies_df DataFrame.\n",
    "#     budget = wiki_movies_df['Budget'].dropna()\n",
    "#     budget = budget.map(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "#     budget = budget.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)\n",
    "#     budget = budget.str.replace(r'\\[\\d+\\]\\s*', '')\n",
    "#     form_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on'\n",
    "#     form_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)'\n",
    "#     wiki_movies_df['budget'] = budget.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "#     wiki_movies_df.drop('Budget', axis=1, inplace=True)\n",
    "    \n",
    "#     # 15. Clean the release date column in the wiki_movies_df DataFrame.\n",
    "#     release_date = wiki_movies_df['Release date'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "#     date_form_one = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s[123]\\d,\\s\\d{4}'\n",
    "#     date_form_two = r'\\d{4}.[01]\\d.[123]\\d'\n",
    "#     date_form_three = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{4}'\n",
    "#     date_form_four = r'\\d{4}'\n",
    "#     release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})', flags=re.IGNORECASE)\n",
    "#     wiki_movies_df['release_date'] = pd.to_datetime(release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})')[0], infer_datetime_format=True)\n",
    "    \n",
    "#     # 16. Clean the running time column in the wiki_movies_df DataFrame.\n",
    "#     running_time = wiki_movies_df['Running time'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "#     running_time_extract = running_time.str.extract(r'(\\d+)\\s*ho?u?r?s?\\s*(\\d*)|(\\d+)\\s*m')\n",
    "#     running_time_extract = running_time_extract.apply(lambda col: pd.to_numeric(col, errors='coerce')).fillna(0)\n",
    "#     wiki_movies_df['running_time'] = running_time_extract.apply(lambda row: row[0]*60 + row[1] if row[2] == 0 else row[2], axis=1)\n",
    "#     wiki_movies_df.drop('Running time', axis=1, inplace=True)\n",
    "    \n",
    "     \n",
    "#     # 2. Clean the Kaggle metadata.\n",
    "#     kaggle_metadata = kaggle_metadata[kaggle_metadata['adult'] == 'False'].drop('adult',axis='columns')\n",
    "#     kaggle_metadata['video'] = kaggle_metadata['video'] == 'True'\n",
    "#     kaggle_metadata['budget'] = kaggle_metadata['budget'].astype(int)\n",
    "#     kaggle_metadata['id'] = pd.to_numeric(kaggle_metadata['id'], errors='raise')\n",
    "#     kaggle_metadata['popularity'] = pd.to_numeric(kaggle_metadata['popularity'], errors='raise')\n",
    "#     kaggle_metadata['release_date'] = pd.to_datetime(kaggle_metadata['release_date'])\n",
    "    \n",
    "#     # 3. Merged the two DataFrames into the movies DataFrame.\n",
    "#     movies_df = pd.merge(wiki_movies_df, kaggle_metadata, on='imdb_id', suffixes=['_wiki','_kaggle'])\n",
    "    \n",
    "#     # 4. Drop unnecessary columns from the merged DataFrame.\n",
    "#     movies_df.drop(columns=['title_wiki','release_date_wiki','Language','Production company(s)'], inplace=True)\n",
    "    \n",
    "#     # 5. Add in the function to fill in the missing Kaggle data.\n",
    "\n",
    "#     def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "#         df[kaggle_column] = df.apply(\n",
    "#             lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column]\n",
    "#             , axis=1)\n",
    "#         df.drop(columns=wiki_column, inplace=True)\n",
    "        \n",
    "#     # 6. Call the function in Step 5 with the DataFrame and columns as the arguments.\n",
    "#     fill_missing_kaggle_data(movies_df, 'runtime', 'running_time')\n",
    "#     fill_missing_kaggle_data(movies_df, 'budget_kaggle', 'budget_wiki')\n",
    "#     fill_missing_kaggle_data(movies_df, 'revenue', 'box_office')\n",
    "#     movies_df\n",
    "\n",
    "#     # 7. Filter the movies DataFrame for specific columns.\n",
    "#     movies_df = movies_df.loc[:, ['imdb_id','id','title_kaggle','original_title','tagline','belongs_to_collection','url','imdb_link',\n",
    "#                        'runtime','budget_kaggle','revenue','release_date_kaggle','popularity','vote_average','vote_count',\n",
    "#                        'genres','original_language','overview','spoken_languages','Country',\n",
    "#                        'production_companies','production_countries','Distributor',\n",
    "#                        'Producer(s)','Director','Starring','Cinematography','Editor(s)','Writer(s)','Composer(s)','Based on'\n",
    "#                       ]]\n",
    "\n",
    "#     # 8. Rename the columns in the movies DataFrame.\n",
    "#     movies_df.rename({'id':'kaggle_id',\n",
    "#                   'title_kaggle':'title',\n",
    "#                   'url':'wikipedia_url',\n",
    "#                   'budget_kaggle':'budget',\n",
    "#                   'release_date_kaggle':'release_date',\n",
    "#                   'Country':'country',\n",
    "#                   'Distributor':'distributor',\n",
    "#                   'Producer(s)':'producers',\n",
    "#                   'Director':'director',\n",
    "#                   'Starring':'starring',\n",
    "#                   'Cinematography':'cinematography',\n",
    "#                   'Editor(s)':'editors',\n",
    "#                   'Writer(s)':'writers',\n",
    "#                   'Composer(s)':'composers',\n",
    "#                   'Based on':'based_on'\n",
    "#                  }, axis='columns', inplace=True)\n",
    "\n",
    "#     # 9. Transform and merge the ratings DataFrame.\n",
    "    \n",
    "#     rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "#                 .rename({'userId':'count'}, axis=1) \\\n",
    "#                 .pivot(index='movieId',columns='rating', values='count')\n",
    "#     rating_counts.columns = ['rating_' + str(col) for col in rating_counts.columns]\n",
    "#     movies_with_ratings_df = pd.merge(movies_df, rating_counts, left_on='kaggle_id', right_index=True, how='left')\n",
    "    \n",
    "#     db_string = f\"postgres://postgres:{db_password}@127.0.0.1:5432/movie_data\"\n",
    "    \n",
    "#     engine = create_engine(db_string)\n",
    "    \n",
    "#     movies_df.to_sql(name='movies', con=engine,if_exists='replace')\n",
    "    \n",
    "#     rows_imported = 0\n",
    "# # get the start_time from time.time()\n",
    "# start_time = time.time()\n",
    "# for data in pd.read_csv(f'{file_dir}ratings.csv', chunksize=1000000):\n",
    "#     print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "#     data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "#     rows_imported += len(data)\n",
    "\n",
    "#     # add elapsed time to final print out\n",
    "#     print(f'Done. {time.time() - start_time} total seconds elapsed')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Create the path to your file directory and variables for the three files.\n",
    "file_dir = '/Users/emily/Desktop/bootcamp_projects/Movies-ETL/'\n",
    "# The Wikipedia data\n",
    "wiki_file = f'{file_dir}/wikipedia.movies.json'\n",
    "# The Kaggle metadata\n",
    "kaggle_file = f'{file_dir}/movies_metadata.csv'\n",
    "# The MovieLens rating data.\n",
    "ratings_file = f'{file_dir}/ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'movies_metadata.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-f8543da1323a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 11. Set the three variables equal to the function created in D1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwiki_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkaggle_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratings_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_transform_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-70-efbb7d3832f4>\u001b[0m in \u001b[0;36mextract_transform_load\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Read in the kaggle metadata and MovieLens ratings CSV files as Pandas DataFrames.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mkaggle_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'movies_metadata.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ratings.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'movies_metadata.csv'"
     ]
    }
   ],
   "source": [
    "# 11. Set the three variables equal to the function created in D1.\n",
    "wiki_file, kaggle_file, ratings_file = extract_transform_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Set the DataFrames from the return statement equal to the file names in Step 11. \n",
    "wiki_movies_df = wiki_file\n",
    "movies_with_ratings_df = kaggle_file\n",
    "movies_df = ratings_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-32d9c285f845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 13. Check the wiki_movies_df DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwiki_movies_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "# 13. Check the wiki_movies_df DataFrame. \n",
    "wiki_movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-e685f2343bae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 14. Check the movies_with_ratings_df DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmovies_with_ratings_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "# 14. Check the movies_with_ratings_df DataFrame.\n",
    "movies_with_ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>kaggle_id</th>\n",
       "      <th>title</th>\n",
       "      <th>original_title</th>\n",
       "      <th>tagline</th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>wikipedia_url</th>\n",
       "      <th>imdb_link</th>\n",
       "      <th>runtime</th>\n",
       "      <th>budget</th>\n",
       "      <th>...</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>distributor</th>\n",
       "      <th>producers</th>\n",
       "      <th>director</th>\n",
       "      <th>starring</th>\n",
       "      <th>cinematography</th>\n",
       "      <th>editors</th>\n",
       "      <th>writers</th>\n",
       "      <th>composers</th>\n",
       "      <th>based_on</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0098987</td>\n",
       "      <td>9548</td>\n",
       "      <td>The Adventures of Ford Fairlane</td>\n",
       "      <td>The Adventures of Ford Fairlane</td>\n",
       "      <td>Kojak. Columbo. Dirty Harry. Wimps.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Adventures_o...</td>\n",
       "      <td>https://www.imdb.com/title/tt0098987/</td>\n",
       "      <td>104.0</td>\n",
       "      <td>49000000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'iso_3166_1': 'US', 'name': 'United States o...</td>\n",
       "      <td>20th Century Fox</td>\n",
       "      <td>[Steve Perry, Joel Silver]</td>\n",
       "      <td>Renny Harlin</td>\n",
       "      <td>[Andrew Dice Clay, Wayne Newton, Priscilla Pre...</td>\n",
       "      <td>Oliver Wood</td>\n",
       "      <td>Michael Tronick</td>\n",
       "      <td>[David Arnott, James Cappe]</td>\n",
       "      <td>[Cliff Eidelman, Yello]</td>\n",
       "      <td>[Characters, by Rex Weiner]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0098994</td>\n",
       "      <td>25501</td>\n",
       "      <td>After Dark, My Sweet</td>\n",
       "      <td>After Dark, My Sweet</td>\n",
       "      <td>All they risked was everything.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://en.wikipedia.org/wiki/After_Dark,_My_S...</td>\n",
       "      <td>https://www.imdb.com/title/tt0098994/</td>\n",
       "      <td>114.0</td>\n",
       "      <td>6000000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'iso_3166_1': 'US', 'name': 'United States o...</td>\n",
       "      <td>Avenue Pictures</td>\n",
       "      <td>[Ric Kidney, Robert Redlin]</td>\n",
       "      <td>James Foley</td>\n",
       "      <td>[Jason Patric, Rachel Ward, Bruce Dern, George...</td>\n",
       "      <td>Mark Plummer</td>\n",
       "      <td>Howard E. Smith</td>\n",
       "      <td>[James Foley, Robert Redlin]</td>\n",
       "      <td>Maurice Jarre</td>\n",
       "      <td>[the novel, After Dark, My Sweet, by, Jim Thom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0099005</td>\n",
       "      <td>11856</td>\n",
       "      <td>Air America</td>\n",
       "      <td>Air America</td>\n",
       "      <td>The few. The proud. The totally insane.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Air_America_(film)</td>\n",
       "      <td>https://www.imdb.com/title/tt0099005/</td>\n",
       "      <td>112.0</td>\n",
       "      <td>35000000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'iso_3166_1': 'US', 'name': 'United States o...</td>\n",
       "      <td>TriStar Pictures</td>\n",
       "      <td>Daniel Melnick</td>\n",
       "      <td>Roger Spottiswoode</td>\n",
       "      <td>[Mel Gibson, Robert Downey Jr., Nancy Travis, ...</td>\n",
       "      <td>Roger Deakins</td>\n",
       "      <td>[John Bloom, Lois Freeman-Fox]</td>\n",
       "      <td>[John Eskow, Richard Rush]</td>\n",
       "      <td>Charles Gross</td>\n",
       "      <td>[Air America, by, Christopher Robbins]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0099012</td>\n",
       "      <td>8217</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Alice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Alice_(1990_film)</td>\n",
       "      <td>https://www.imdb.com/title/tt0099012/</td>\n",
       "      <td>102.0</td>\n",
       "      <td>12000000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'iso_3166_1': 'US', 'name': 'United States o...</td>\n",
       "      <td>Orion Pictures</td>\n",
       "      <td>Robert Greenhut</td>\n",
       "      <td>Woody Allen</td>\n",
       "      <td>[Alec Baldwin, Blythe Danner, Judy Davis, Mia ...</td>\n",
       "      <td>Carlo Di Palma</td>\n",
       "      <td>Susan E. Morse</td>\n",
       "      <td>Woody Allen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0099018</td>\n",
       "      <td>25943</td>\n",
       "      <td>Almost an Angel</td>\n",
       "      <td>Almost an Angel</td>\n",
       "      <td>Who does he think he is?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Almost_an_Angel</td>\n",
       "      <td>https://www.imdb.com/title/tt0099018/</td>\n",
       "      <td>95.0</td>\n",
       "      <td>25000000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'iso_3166_1': 'US', 'name': 'United States o...</td>\n",
       "      <td>Paramount Pictures</td>\n",
       "      <td>John Cornell</td>\n",
       "      <td>John Cornell</td>\n",
       "      <td>[Paul Hogan, Elias Koteas, Linda Kozlowski]</td>\n",
       "      <td>Russell Boyd</td>\n",
       "      <td>David Stiven</td>\n",
       "      <td>Paul Hogan</td>\n",
       "      <td>Maurice Jarre</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     imdb_id  kaggle_id                            title  \\\n",
       "0  tt0098987       9548  The Adventures of Ford Fairlane   \n",
       "1  tt0098994      25501             After Dark, My Sweet   \n",
       "2  tt0099005      11856                      Air America   \n",
       "3  tt0099012       8217                            Alice   \n",
       "4  tt0099018      25943                  Almost an Angel   \n",
       "\n",
       "                    original_title                                  tagline  \\\n",
       "0  The Adventures of Ford Fairlane      Kojak. Columbo. Dirty Harry. Wimps.   \n",
       "1             After Dark, My Sweet          All they risked was everything.   \n",
       "2                      Air America  The few. The proud. The totally insane.   \n",
       "3                            Alice                                      NaN   \n",
       "4                  Almost an Angel                 Who does he think he is?   \n",
       "\n",
       "  belongs_to_collection                                      wikipedia_url  \\\n",
       "0                   NaN  https://en.wikipedia.org/wiki/The_Adventures_o...   \n",
       "1                   NaN  https://en.wikipedia.org/wiki/After_Dark,_My_S...   \n",
       "2                   NaN   https://en.wikipedia.org/wiki/Air_America_(film)   \n",
       "3                   NaN    https://en.wikipedia.org/wiki/Alice_(1990_film)   \n",
       "4                   NaN      https://en.wikipedia.org/wiki/Almost_an_Angel   \n",
       "\n",
       "                               imdb_link  runtime      budget  ...  \\\n",
       "0  https://www.imdb.com/title/tt0098987/    104.0  49000000.0  ...   \n",
       "1  https://www.imdb.com/title/tt0098994/    114.0   6000000.0  ...   \n",
       "2  https://www.imdb.com/title/tt0099005/    112.0  35000000.0  ...   \n",
       "3  https://www.imdb.com/title/tt0099012/    102.0  12000000.0  ...   \n",
       "4  https://www.imdb.com/title/tt0099018/     95.0  25000000.0  ...   \n",
       "\n",
       "                                production_countries         distributor  \\\n",
       "0  [{'iso_3166_1': 'US', 'name': 'United States o...    20th Century Fox   \n",
       "1  [{'iso_3166_1': 'US', 'name': 'United States o...     Avenue Pictures   \n",
       "2  [{'iso_3166_1': 'US', 'name': 'United States o...    TriStar Pictures   \n",
       "3  [{'iso_3166_1': 'US', 'name': 'United States o...      Orion Pictures   \n",
       "4  [{'iso_3166_1': 'US', 'name': 'United States o...  Paramount Pictures   \n",
       "\n",
       "                     producers            director  \\\n",
       "0   [Steve Perry, Joel Silver]        Renny Harlin   \n",
       "1  [Ric Kidney, Robert Redlin]         James Foley   \n",
       "2               Daniel Melnick  Roger Spottiswoode   \n",
       "3              Robert Greenhut         Woody Allen   \n",
       "4                 John Cornell        John Cornell   \n",
       "\n",
       "                                            starring  cinematography  \\\n",
       "0  [Andrew Dice Clay, Wayne Newton, Priscilla Pre...     Oliver Wood   \n",
       "1  [Jason Patric, Rachel Ward, Bruce Dern, George...    Mark Plummer   \n",
       "2  [Mel Gibson, Robert Downey Jr., Nancy Travis, ...   Roger Deakins   \n",
       "3  [Alec Baldwin, Blythe Danner, Judy Davis, Mia ...  Carlo Di Palma   \n",
       "4        [Paul Hogan, Elias Koteas, Linda Kozlowski]    Russell Boyd   \n",
       "\n",
       "                          editors                       writers  \\\n",
       "0                 Michael Tronick   [David Arnott, James Cappe]   \n",
       "1                 Howard E. Smith  [James Foley, Robert Redlin]   \n",
       "2  [John Bloom, Lois Freeman-Fox]    [John Eskow, Richard Rush]   \n",
       "3                  Susan E. Morse                   Woody Allen   \n",
       "4                    David Stiven                    Paul Hogan   \n",
       "\n",
       "                 composers                                           based_on  \n",
       "0  [Cliff Eidelman, Yello]                        [Characters, by Rex Weiner]  \n",
       "1            Maurice Jarre  [the novel, After Dark, My Sweet, by, Jim Thom...  \n",
       "2            Charles Gross             [Air America, by, Christopher Robbins]  \n",
       "3                      NaN                                                NaN  \n",
       "4            Maurice Jarre                                                NaN  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 15. Check the movies_df DataFrame. \n",
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing rows 0 to 1000000...Done. 182.5992612838745 total seconds elapsed\n",
      "importing rows 1000000 to 2000000...Done. 347.42689514160156 total seconds elapsed\n",
      "importing rows 2000000 to 3000000...Done. 3293.3160030841827 total seconds elapsed\n",
      "importing rows 3000000 to 4000000...Done. 3497.730921983719 total seconds elapsed\n",
      "importing rows 4000000 to 5000000...Done. 3722.9211881160736 total seconds elapsed\n",
      "importing rows 5000000 to 6000000...Done. 3965.814824104309 total seconds elapsed\n",
      "importing rows 6000000 to 7000000...Done. 4186.645003080368 total seconds elapsed\n",
      "importing rows 7000000 to 8000000...Done. 4391.865693330765 total seconds elapsed\n",
      "importing rows 8000000 to 9000000...Done. 4575.404323101044 total seconds elapsed\n",
      "importing rows 9000000 to 10000000...Done. 5002.547312259674 total seconds elapsed\n",
      "importing rows 10000000 to 11000000...Done. 5208.639186382294 total seconds elapsed\n",
      "importing rows 11000000 to 12000000...Done. 5385.706453084946 total seconds elapsed\n",
      "importing rows 12000000 to 13000000...Done. 5931.0171110630035 total seconds elapsed\n",
      "importing rows 13000000 to 14000000...Done. 6131.904808044434 total seconds elapsed\n",
      "importing rows 14000000 to 15000000...Done. 6307.61366724968 total seconds elapsed\n",
      "importing rows 15000000 to 16000000...Done. 6819.822190284729 total seconds elapsed\n",
      "importing rows 16000000 to 17000000...Done. 7018.1660323143005 total seconds elapsed\n",
      "importing rows 17000000 to 18000000...Done. 7224.419512271881 total seconds elapsed\n",
      "importing rows 18000000 to 19000000...Done. 7475.201447248459 total seconds elapsed\n",
      "importing rows 19000000 to 20000000...Done. 7734.958257198334 total seconds elapsed\n",
      "importing rows 20000000 to 21000000..."
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "(psycopg2.errors.DiskFull) could not extend file \"base/16635/16859\": No space left on device\nHINT:  Check free disk space.\n\n[SQL: INSERT INTO ratings (index, \"userId\", \"movieId\", rating, timestamp) VALUES (%(index)s, %(userId)s, %(movieId)s, %(rating)s, %(timestamp)s)]\n[parameters: ({'index': 20000000, 'userId': 207965, 'movieId': 89864, 'rating': 4.0, 'timestamp': 1324774428}, {'index': 20000001, 'userId': 207966, 'movieId': 34, 'rating': 5.0, 'timestamp': 916502599}, {'index': 20000002, 'userId': 207966, 'movieId': 260, 'rating': 5.0, 'timestamp': 916502786}, {'index': 20000003, 'userId': 207966, 'movieId': 509, 'rating': 3.0, 'timestamp': 916503034}, {'index': 20000004, 'userId': 207966, 'movieId': 534, 'rating': 4.0, 'timestamp': 916502380}, {'index': 20000005, 'userId': 207966, 'movieId': 919, 'rating': 5.0, 'timestamp': 916502667}, {'index': 20000006, 'userId': 207966, 'movieId': 924, 'rating': 5.0, 'timestamp': 916502298}, {'index': 20000007, 'userId': 207966, 'movieId': 1090, 'rating': 4.0, 'timestamp': 916502728}  ... displaying 10 of 1000000 total bound parameter sets ...  {'index': 20999998, 'userId': 218237, 'movieId': 4011, 'rating': 3.0, 'timestamp': 1462582495}, {'index': 20999999, 'userId': 218237, 'movieId': 4226, 'rating': 4.0, 'timestamp': 1462585028})]\n(Background on this error at: http://sqlalche.me/e/13/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDiskFull\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt_handled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m                     self.dialect.do_executemany(\n\u001b[0m\u001b[1;32m   1257\u001b[0m                         \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py\u001b[0m in \u001b[0;36mdo_executemany\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutemany_mode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mEXECUTEMANY_DEFAULT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutemany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDiskFull\u001b[0m: could not extend file \"base/16635/16859\": No space left on device\nHINT:  Check free disk space.\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-d6b9148778c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{file_dir}ratings.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'importing rows {rows_imported} to {rows_imported + len(data)}...'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ratings'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mrows_imported\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m   2603\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msql\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2605\u001b[0;31m         sql.to_sql(\n\u001b[0m\u001b[1;32m   2606\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2607\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m    587\u001b[0m         )\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m     pandas_sql.to_sql(\n\u001b[0m\u001b[1;32m    590\u001b[0m         \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m   1404\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf cannot be used with MySQL\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[0;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method)\u001b[0m\n\u001b[1;32m   1396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1398\u001b[0;31m             \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSQLAlchemyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m             \u001b[0;31m# GH34431\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, chunksize, method)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m                 \u001b[0mchunk_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m                 \u001b[0mexec_insert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m     def _query_iterator(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36m_execute_insert\u001b[0;34m(self, conn, keys, data_iter)\u001b[0m\n\u001b[1;32m    745\u001b[0m         \"\"\"\n\u001b[1;32m    746\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_insert_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, object_, *multiparams, **params)\u001b[0m\n\u001b[1;32m   1009\u001b[0m             )\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sqlalchemy/sql/elements.py\u001b[0m in \u001b[0;36m_execute_on_connection\u001b[0;34m(self, connection, multiparams, params)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_on_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_execution\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_clauseelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectNotExecutableError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_clauseelement\u001b[0;34m(self, elem, multiparams, params)\u001b[0m\n\u001b[1;32m   1122\u001b[0m             )\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         ret = self._execute_context(\n\u001b[0m\u001b[1;32m   1125\u001b[0m             \u001b[0mdialect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0mdialect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_ctx_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_compiled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m             self._handle_dbapi_exception(\n\u001b[0m\u001b[1;32m   1317\u001b[0m                 \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[1;32m   1508\u001b[0m                 \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewraise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_traceback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mshould_wrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1510\u001b[0;31m                 util.raise_(\n\u001b[0m\u001b[1;32m   1511\u001b[0m                     \u001b[0msqlalchemy_exception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_traceback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m                 )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sqlalchemy/util/compat.py\u001b[0m in \u001b[0;36mraise_\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;31m# credit to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1254\u001b[0m                             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt_handled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m                     self.dialect.do_executemany(\n\u001b[0m\u001b[1;32m   1257\u001b[0m                         \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m                     )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py\u001b[0m in \u001b[0;36mdo_executemany\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    871\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_executemany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutemany_mode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mEXECUTEMANY_DEFAULT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutemany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: (psycopg2.errors.DiskFull) could not extend file \"base/16635/16859\": No space left on device\nHINT:  Check free disk space.\n\n[SQL: INSERT INTO ratings (index, \"userId\", \"movieId\", rating, timestamp) VALUES (%(index)s, %(userId)s, %(movieId)s, %(rating)s, %(timestamp)s)]\n[parameters: ({'index': 20000000, 'userId': 207965, 'movieId': 89864, 'rating': 4.0, 'timestamp': 1324774428}, {'index': 20000001, 'userId': 207966, 'movieId': 34, 'rating': 5.0, 'timestamp': 916502599}, {'index': 20000002, 'userId': 207966, 'movieId': 260, 'rating': 5.0, 'timestamp': 916502786}, {'index': 20000003, 'userId': 207966, 'movieId': 509, 'rating': 3.0, 'timestamp': 916503034}, {'index': 20000004, 'userId': 207966, 'movieId': 534, 'rating': 4.0, 'timestamp': 916502380}, {'index': 20000005, 'userId': 207966, 'movieId': 919, 'rating': 5.0, 'timestamp': 916502667}, {'index': 20000006, 'userId': 207966, 'movieId': 924, 'rating': 5.0, 'timestamp': 916502298}, {'index': 20000007, 'userId': 207966, 'movieId': 1090, 'rating': 4.0, 'timestamp': 916502728}  ... displaying 10 of 1000000 total bound parameter sets ...  {'index': 20999998, 'userId': 218237, 'movieId': 4011, 'rating': 3.0, 'timestamp': 1462582495}, {'index': 20999999, 'userId': 218237, 'movieId': 4226, 'rating': 4.0, 'timestamp': 1462585028})]\n(Background on this error at: http://sqlalche.me/e/13/e3q8)"
     ]
    }
   ],
   "source": [
    "rows_imported = 0\n",
    "# get the start_time from time.time()\n",
    "start_time = time.time()\n",
    "for data in pd.read_csv(f'{file_dir}ratings.csv', chunksize=1000000):\n",
    "    print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "    data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "    rows_imported += len(data)\n",
    "\n",
    "    # add elapsed time to final print out\n",
    "    print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
